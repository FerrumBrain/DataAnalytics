{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e9b7c3b2ea6e90",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b0a904b39aaa7",
   "metadata": {},
   "source": [
    "For our project We are analyzing the dataset of all flights that departed from the three main New York City airports in 2023. For this project We will be analyzing all the features of the dataset and from the auxiliary ones. For the regression task our target variable is 'arr_delay' and for classification task we have chosen flight cancellation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dea59059fbd1f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d74de3becc52",
   "metadata": {},
   "source": [
    "Overview of datasets:\n",
    "* airlines.csv\n",
    "    * `carrier` -- Two-letter airline code.\n",
    "    * `name` -- Full name of the airline.\n",
    "\n",
    "* airports.csv\n",
    "    * `faa` -- FAA code of the airport (e.g., JFK for John F. Kennedy).\n",
    "    * `name` -- Name of the airport.\n",
    "    * `lat` -- Latitude of the airport (decimal format).\n",
    "    * `lon` -- Longitude of the airport (decimal format).\n",
    "    * `alt` -- Altitude of the airport in feet.\n",
    "    * `tz` -- Time zone offset (UTC offset). Can be `null`\n",
    "    * `dst` -- Daylight saving time indicator (`A` = Active, etc.). Can be `null` but it is `null` iff `tz` is `null`\n",
    "    * `tzone` -- IANA time zone name (e.g., America/New_York). Can be `null` but it is `null` in superset of cases when `tz` is null, therefore can be filled sometimes with already existing values\n",
    "\n",
    "* flights.csv\n",
    "    * `year` -- Year of the flight. Always `2023`, therefore dropped\n",
    "    * `month` -- Month of the flight.\n",
    "    * `day` -- Day of the flight.\n",
    "    * `dep_time` -- Actual departure time (local, in military format). Can be `null`\n",
    "    * `sched_dep_time` -- Scheduled departure time (local, in military format).\n",
    "    * `dep_delay` -- Departure delay in minutes (negative if early). Can be `null` iff `dep_time` is null\n",
    "    * `arr_time` -- Actual arrival time (local, in military format). Can be `null` in superset of cases when `dep_delay` is `null`. If it is `null` we assume that flight was cancelled.\n",
    "    * `sched_arr_time` -- Scheduled arrival time (local, in military format).\n",
    "    * `arr_delay` -- Arrival delay in minutes (negative if early). Can be `null` in superset of cases when `arr_time` is `null`. Can be filled in as difference between `sched_arr_time` and `arr_time`\n",
    "    * `carrier` -- Two-letter airline code (links to `airlines.csv`).\n",
    "    * `flight` -- Flight number.\n",
    "    * `tailnum` -- Aircraft tail number (links to `planes.csv`). Cna be `null` but number is small => can be dropped.\n",
    "    * `origin` -- Origin airport code (links to `airports.csv`).\n",
    "    * `dest` -- Destination airport code (links to `airports.csv`).\n",
    "    * `air_time` -- Total air time in minutes. Can be `null` iff `arr_delay` is null but we decided not to use this column => can be ignored.\n",
    "    * `distance` -- Distance of the flight in miles.\n",
    "    * `hour` -- Scheduled departure hour (derived from `sched_dep_time`).\n",
    "    * `minute` -- Scheduled departure minute (derived from `sched_dep_time`).\n",
    "    * `time_hour` -- Rounded time to the nearest hour (useful for joining with weather data).\n",
    "\n",
    "* planes.csv\n",
    "    * `tailnum` -- Unique aircraft identifier (matches with `flights.tailnum`).\n",
    "    * `year` -- Year the plane was manufactured. Can be `null` but it is quite rare case, therefore can be dropped\n",
    "    * `type` -- Aircraft type (e.g., \"Fixed wing multi engine\").\n",
    "    * `manufacturer` -- Manufacturer of the aircraft (e.g., Boeing, Airbus).\n",
    "    * `model` -- Model of the aircraft (e.g., \"A320\").\n",
    "    * `engines` -- Number of engines on the aircraft.\n",
    "    * `seats` -- Number of passenger seats on the plane. Always `0`, therefore dropped\n",
    "    * `speed` -- Typical cruise speed (if available; may be NaN).\n",
    "    * `engine` -- Type of engine (e.g., \"Turbo-fan\").\n",
    "\n",
    "* weather.csv\n",
    "    * `origin` -- Airport code where the weather data is recorded (links to `airports.faa`).\n",
    "    * `year` -- Year of the weather record.\n",
    "    * `month` -- Month of the weather record.\n",
    "    * `day` -- Day of the weather record.\n",
    "    * `hour` -- Hour of the weather observation (local).\n",
    "    * `temp` -- Temperature in degrees Fahrenheit. Can be `null` in superset of cases when `precip` is `null`\n",
    "    * `dewp` -- Dewpoint temperature in degrees Fahrenheit. Can be `null` iff `temp` is `null`\n",
    "    * `humid` -- Relative humidity (%). Can be `null` iff `temp` is `null`\n",
    "    * `wind_dir` -- Wind direction in degrees (0° = North, 90° = East, etc.).\n",
    "    * `wind_speed` -- Wind speed in mph.\n",
    "    * `wind_gust` -- Wind gust speed in mph (if available).\n",
    "    * `precip` -- Precipitation in inches. Can be `null`.\n",
    "    * `pressure` -- Atmospheric pressure in millibars (if available). Can be `null` in superset of cases when `temp` is `null`\n",
    "    * `visib` -- Visibility in miles.\n",
    "    * `time_hour` -- Time of the observation rounded to the nearest hour (links to `flights.time_hour`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ae81969987003",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd71e857-98ed-4784-8ac2-97ce2f9d4b5a",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fc266a182b277f8",
   "metadata": {},
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7b71c40295a8994",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "213e14fd285a4625",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "329f9868b1334081",
   "metadata": {},
   "source": [
    "def read_csv(filename):\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        header=0,                     # Use the first row as the header\n",
    "        delimiter=';',                # Use semicolon as the main delimiter\n",
    "        decimal=',',                  # Specify that commas are used as decimals\n",
    "        quotechar='\"',                # Handle quotes around strings\n",
    "        skipinitialspace=True,        # Skip spaces after delimiters\n",
    "    )\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "airlines = read_csv('airlines.csv')\n",
    "airports = read_csv('airports.csv')\n",
    "planes = read_csv('planes.csv')\n",
    "flights = read_csv('flights.csv')\n",
    "weather = read_csv('weather.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some columns have float type instead of int, we need to change it",
   "id": "43ed8abfa6376ee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "id": "8cfd61857abe039a",
   "source": [
    "planes['year'] = planes['year'].astype('Int64')\n",
    "flights['dep_time'] = flights['dep_time'].astype('Int64')\n",
    "flights['dep_delay'] = flights['dep_delay'].astype('Int64')\n",
    "flights['arr_time'] = flights['arr_time'].astype('Int64')\n",
    "flights['arr_delay'] = flights['arr_delay'].astype('Int64')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b594aaacb8ba179",
   "metadata": {},
   "source": [
    "def check_for_nulls(df):\n",
    "    res = {}\n",
    "    for c in df.columns:\n",
    "        if df[df[c].isnull()].shape[0] != 0:\n",
    "            res[c] = df[df[c].isnull()].shape[0]\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's analyze nulls in datasets",
   "id": "1ae98d764e8740c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "id": "1739a985a9dae7f7",
   "source": [
    "print(\"Nullable columns in airlines:\", check_for_nulls(airlines))\n",
    "print(\"Nullable columns in airports:\", check_for_nulls(airports))\n",
    "print(\"Nullable columns in planes:\", check_for_nulls(planes))\n",
    "print(\"Nullable columns in flights:\", check_for_nulls(flights))\n",
    "print(\"Nullable columns in weather:\", check_for_nulls(weather))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "After careful analysis we found dependencies that were mentioned in data overview",
   "id": "2d6f054fbb426fe5",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean Datasets",
   "id": "dfb9394633c358f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have observed, that for some flights even though the scheduled departure and arrival times are present, the actual departure and arrival times are absent (i.e., NaN). We have deduced, that this can be interpreted as a cancellation of said flight. We have checked this hypothesis with publicly available information, and it was confirmed.",
   "id": "fa5059b37e6fd31b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to clean each dataset from nulls according to our expectations of data",
   "id": "2ecf0b2578c33f55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "### Clean Datasets",
   "id": "dbde2c82f1211976",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b705d8f96fc2fb5e",
   "metadata": {},
   "source": [
    "def clear_airports(df): # fill `tzone` if it can be filled, drop otherwise. If `tz` is unknown, drop\n",
    "    df = df.dropna(subset=['tz', 'dst'])\n",
    "\n",
    "    for idx, row in df[df['tzone'].isnull()].iterrows():\n",
    "        matching_row = df[(df['tz'] == row['tz']) & pd.notnull(df['tzone'])]\n",
    "        if not matching_row.empty:\n",
    "            df.at[idx, 'tzone'] = matching_row['tzone'].iloc[0]\n",
    "        else:\n",
    "            df = df.drop(idx)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f920512d45adf7",
   "metadata": {},
   "source": [
    "def clear_planes(df): # drop all planes if `year` is unknown\n",
    "    return df.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a9b2b847cac2208",
   "metadata": {},
   "source": [
    "def clear_flights(df): # drop if `tailnum` is unknown, fix military format of time, fill air_time where needed\n",
    "    df = df.dropna(subset=['tailnum'])\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        df.at[idx, 'dep_time'] = df.at[idx, 'dep_time'] % 100 + (df.at[idx, 'dep_time'] // 100) * 60\n",
    "        df.at[idx, 'arr_time'] = df.at[idx, 'arr_time'] % 100 + (df.at[idx, 'arr_time'] // 100) * 60\n",
    "        df.at[idx, 'sched_dep_time'] = df.at[idx, 'sched_dep_time'] % 100 + (df.at[idx, 'sched_dep_time'] // 100) * 60\n",
    "        df.at[idx, 'sched_arr_time'] = df.at[idx, 'sched_arr_time'] % 100 + (df.at[idx, 'sched_arr_time'] // 100) * 60\n",
    "\n",
    "    for idx, row in df[df['air_time'].isnull()].iterrows():\n",
    "        # df.at[idx, 'arr_delay'] = df['arr_time'].iloc[0] - df['sched_arr_time'].iloc[0]\n",
    "        df.at[idx, 'air_time'] = df['arr_time'].iloc[0] - df['dep_time'].iloc[0]\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fd53bb08c95a1a2",
   "metadata": {},
   "source": [
    "def clear_weather(df): # here we ignore all nulls and keep them as is\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777d3956c48e52a0",
   "metadata": {},
   "source": [
    "clean_airlines = airlines\n",
    "clean_airports = clear_airports(airports.copy())\n",
    "clean_planes = clear_planes(planes.copy())\n",
    "clean_flights = clear_flights(flights.copy())\n",
    "clean_weather = clear_weather(weather.copy())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fdd007585f4859d9",
   "metadata": {},
   "source": [
    "print(\"Nullable columns in airlines:\", check_for_nulls(clean_airlines))\n",
    "print(\"Nullable columns in airports:\", check_for_nulls(clean_airports))\n",
    "print(\"Nullable columns in planes:\", check_for_nulls(clean_planes))\n",
    "print(\"Nullable columns in flights:\", check_for_nulls(clean_flights))\n",
    "print(\"Nullable columns in weather:\", check_for_nulls(clean_weather))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dcef2e2c26d0856",
   "metadata": {},
   "source": [
    "print(\"Airports old vs new:\", airports.shape, clean_airports.shape)\n",
    "print(\"Planes old vs new:\", planes.shape, clean_planes.shape)\n",
    "print(\"Flights old vs new:\", flights.shape, clean_flights.shape)\n",
    "print(\"Weather old vs new:\", weather.shape, clean_weather.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9442ea2b99b4f0b5",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to merge all data together: add info about airports, planes and weather in the origin airport",
   "id": "a28a6b962a6d315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "id": "5b4b9aa9cadcc7ea",
   "source": [
    "# merge flights with info about origin airport\n",
    "flights_with_origin_info = pd.merge(\n",
    "    clean_flights,\n",
    "    clean_airports,\n",
    "    how='left',\n",
    "    left_on='origin',\n",
    "    right_on='faa',\n",
    "    suffixes=('', '_origin')\n",
    ")\n",
    "\n",
    "# merge obtained dataset with info about destination airport\n",
    "flights_with_origin_and_dest = pd.merge(\n",
    "    flights_with_origin_info,\n",
    "    clean_airports,\n",
    "    how='inner',\n",
    "    left_on='dest',\n",
    "    right_on='faa',\n",
    "    suffixes=('_origin', '_dest')\n",
    ")\n",
    "\n",
    "# merge obtained dataset with info about plane\n",
    "flights_with_origin_dest_and_planes = pd.merge(\n",
    "    flights_with_origin_and_dest,\n",
    "    clean_planes,\n",
    "    how='inner',\n",
    "    left_on=['tailnum'],\n",
    "    right_on=['tailnum'],\n",
    "    suffixes=('', '_plane')\n",
    ")\n",
    "\n",
    "# merge obtained dataset with info about weather at the departure time\n",
    "merged_df = pd.merge(\n",
    "    flights_with_origin_dest_and_planes,\n",
    "    clean_weather,\n",
    "    how='left',\n",
    "    left_on=['origin', 'year', 'month', 'day', 'hour'],\n",
    "    right_on=['origin', 'year', 'month', 'day', 'hour'],\n",
    "    suffixes=('', '_weather')\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f859e41d9a8c5ca",
   "metadata": {},
   "source": [
    "merged_df['year_plane'] = merged_df['year'].astype(int)\n",
    "merged_df['is_cancelled'] = merged_df['arr_time'].isnull()\n",
    "\n",
    "# drop columns that are not needed for classification and regression\n",
    "merged_df = merged_df.drop(['arr_time', 'flight', 'tailnum', 'air_time', 'time_hour', 'faa_origin', 'name_origin', 'lat_origin', 'lon_origin', 'alt_origin', 'tz_origin', 'dst_origin', 'tzone_origin', 'faa_dest', 'name_dest', 'dst_dest', 'tzone_dest', 'time_hour_weather', 'year', 'speed', 'hour', 'minute'], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbfe28438b12cd60",
   "metadata": {},
   "source": [
    "merged_df.to_csv(\"merged_df.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4a1878e9fc45c2d",
   "metadata": {},
   "source": [
    "# split into 3 datasets depending on origin airport\n",
    "ewr_df = merged_df[merged_df['origin'] == 'EWR'].drop('origin', axis=1)\n",
    "jfk_df = merged_df[merged_df['origin'] == 'JFK'].drop('origin', axis=1)\n",
    "lga_df = merged_df[merged_df['origin'] == 'LGA'].drop('origin', axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "535a51f2a62daf79",
   "metadata": {},
   "source": [
    "# process each dataset and create datasets for both classification and regression task\n",
    "ewr_delay_df = ewr_df.drop('is_cancelled', axis=1).dropna(subset=['arr_delay', 'dep_delay', 'dep_time'])\n",
    "ewr_cancel_df = ewr_df.drop(['dep_delay', 'arr_delay'], axis=1)\n",
    "jfk_delay_df = jfk_df.drop('is_cancelled', axis=1).dropna(subset=['arr_delay', 'dep_delay', 'dep_time'])\n",
    "jfk_cancel_df = jfk_df.drop(['dep_delay', 'arr_delay'], axis=1)\n",
    "lga_delay_df = lga_df.drop('is_cancelled', axis=1).dropna(subset=['arr_delay', 'dep_delay', 'dep_time'])\n",
    "lga_cancel_df = lga_df.drop(['dep_delay', 'arr_delay'], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "948757bf5154255a",
   "metadata": {},
   "source": [
    "ewr_delay_df.to_csv(\"ewr_delay.csv\", index=False)\n",
    "jfk_delay_df.to_csv(\"jfk_delay.csv\", index=False)\n",
    "lga_delay_df.to_csv(\"lga_delay.csv\", index=False)\n",
    "ewr_cancel_df.to_csv(\"ewr_cancel.csv\", index=False)\n",
    "jfk_cancel_df.to_csv(\"jfk_cancel.csv\", index=False)\n",
    "lga_cancel_df.to_csv(\"lga_cancel.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e29e76b1654b85e",
   "metadata": {},
   "source": [
    "## Exporatory Data Analysis\n",
    "### Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65253a49536e8924",
   "metadata": {},
   "source": [
    "First, let's take a general look over the data distribution. To start with, how many flights are done each month and how delayed flights are distributed over them."
   ]
  },
  {
   "cell_type": "code",
   "id": "cfe44587b754d536",
   "metadata": {},
   "source": [
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b314e1178cc3489",
   "metadata": {},
   "source": [
    "monthly_flights = merged_df.groupby('month').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_flights.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights per Month', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=[f'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5caac923d18a3f4",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['arr_delay'] > 15) | (merged_df['dep_delay'] > 15)]\n",
    "\n",
    "monthly_delays = flights_delayed.groupby('month').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Delayed > 15 Minutes per Month', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=[f'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67582c6aee5f6870",
   "metadata": {},
   "source": [
    "From the plot above we may try to derive some conclusions:\n",
    "- It is clear that the most delays occur in the peak summer months June and July, which may be due to overall increased air traffic, congestions of airports and weather conditions\n",
    "- Similarly, autumn months, when the travels are less frequent, are less affected by delays\n",
    "\n",
    "To analyse this further, we can try to look into differences between arr_delay and dep_delay:"
   ]
  },
  {
   "cell_type": "code",
   "id": "164ab6a94aaa0c69",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['dep_delay'] > 15)]\n",
    "\n",
    "monthly_delays = flights_delayed.groupby('month').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Departed > 15 Minutes than expected per Month', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=[f'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eeacafe1aa3e666b",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['arr_delay'] > 15)]\n",
    "\n",
    "monthly_delays = flights_delayed.groupby('month').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Arrived > 15 Minutes than expected per Month', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=[f'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3594c737ba539818",
   "metadata": {},
   "source": [
    "The plots show that in some months there is a tendency in delayed departures, but not at the identified peak months. Let's see how delayed arrivals are distributed over the source airports:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0da4d1790375cc3",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['arr_delay'] > 15) | (merged_df['dep_delay'] > 15)]\n",
    "\n",
    "delays_by_destination = flights_delayed.groupby('origin').size().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "delays_by_destination.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Delayed Flights (> 15 Minutes) by Origin Airport', fontsize=16)\n",
    "plt.xlabel('Destination Airport', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ab562cc5055d8cb",
   "metadata": {},
   "source": [
    "And what are mean delays in them:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f051a5a7cb22fbc2",
   "metadata": {},
   "source": [
    "flights_delayed.groupby('origin')[['dep_delay', 'arr_delay']].mean().sort_values('arr_delay', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e99483889996e9d",
   "metadata": {},
   "source": [
    "There is no significant difference between the origin airports in terms of delays, let's investigate the destination airports in the similar way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a03075572df5745d",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['arr_delay'] > 15) | (merged_df['dep_delay'] > 15)]\n",
    "\n",
    "delays_by_destination = flights_delayed.groupby('dest').size().sort_values(ascending=False)\n",
    "delays_by_destination = delays_by_destination[delays_by_destination >= 500]\n",
    "\n",
    "# Step 3: Plot the result\n",
    "plt.figure(figsize=(14, 7))\n",
    "delays_by_destination.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Delayed Flights (> 15 Minutes) by Destination Airport (>500 delays over year)', fontsize=16)\n",
    "plt.xlabel('Destination Airport', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e68b1450a4295cdc",
   "metadata": {},
   "source": [
    "The above plot does not contain destination airpots with less than 500 delayed flights over the year (for visual purposes). We can clearly observe a skewed distribution with several airports constituting to the most of the delays. To indicate it more formally, we can investigate cumulative percentage over Pareto threshold:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5676073f8a5997c",
   "metadata": {},
   "source": [
    "delays_by_destination = flights_delayed.groupby('dest').size()\n",
    "delays_by_destination = delays_by_destination[delays_by_destination >= 100].sort_values(ascending=False)\n",
    "\n",
    "cumulative_delays = delays_by_destination.cumsum()\n",
    "cumulative_percentage = 100 * cumulative_delays / cumulative_delays.iloc[-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_percentage, marker='o', color='skyblue', label='Cumulative Percentage')\n",
    "plt.axhline(y=80, color='r', linestyle='--', label='80% Threshold')\n",
    "\n",
    "plt.title('Cumulative Percentage of Delays by Destination Airport', fontsize=16)\n",
    "plt.xlabel('Destination Airport (sorted by delay frequency)', fontsize=12)\n",
    "plt.ylabel('Cumulative Percentage (%)', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(len(cumulative_percentage)), labels=cumulative_percentage.index, rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pareto_threshold = (cumulative_percentage <= 80).sum()\n",
    "print(f\"Number of destinations accounting for 80% of delays: {pareto_threshold} out of {len(delays_by_destination)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cba881069c84065",
   "metadata": {},
   "source": [
    "With this, we see that around 20-30% of the airports (on the left) contribute most to the delays: after reaching 80% the curve significantly flattens. \n",
    "\n",
    "Similarly, by observing amount of flights for each hour of the day versus delayed flights we can see that evening hours contribute more to the delays:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd0e6071836db82",
   "metadata": {},
   "source": [
    "flights_delayed = merged_df[(merged_df['arr_delay'] > 15) | (merged_df['dep_delay'] > 15)]\n",
    "\n",
    "flights_delayed['dep_hour'] = flights_delayed['dep_time'] // 60\n",
    "\n",
    "hourly_delays = flights_delayed.groupby('dep_hour').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Delayed > 15 Minutes per Hour', fontsize=14)\n",
    "plt.xlabel('Hour of the Day', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(24), labels=[f'{hour}:00' for hour in range(24)], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cafd88daf301ae34",
   "metadata": {},
   "source": [
    "all_flights = merged_df\n",
    "all_flights['dep_hour'] = all_flights['dep_time'] // 60\n",
    "\n",
    "hourly_all_delays = all_flights.groupby('dep_hour').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_all_delays.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Delayed > 15 Minutes per Hour', fontsize=14)\n",
    "plt.xlabel('Hour of the Day', fontsize=12)\n",
    "plt.ylabel('Number of Delayed Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(24), labels=[f'{hour}:00' for hour in range(24)], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12b1398ac19b0900",
   "metadata": {},
   "source": [
    "Which can be for various reasons:\n",
    "- Operational congestion of accumulating delays and mismanagements throughout the day (thus the increasing of the amount of delays) \n",
    "- Less space for adjustments \n",
    "- Unequal staffing or external conditions like weather and decreased visibility in the evening hours\n",
    "\n",
    "The weather data for the dataset mostly lacks precise temp/humidity/etc data. We can try to observe the distribution of existing to understand whether this data can be reliably used in the future investigation:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd1cd97c1fcdce32",
   "metadata": {},
   "source": [
    "airports = [\"JFK\", \"EWR\", \"LGA\"]\n",
    "\n",
    "for airport in airports:\n",
    "    filtered_weather = clean_weather[clean_weather['origin'].eq(airport) \n",
    "                                     & clean_weather['temp'].notnull() \n",
    "                                     & clean_weather['dewp'].notnull() \n",
    "                                     & clean_weather['humid'].notnull()]\n",
    "    \n",
    "    filtered_weather['date'] = pd.to_datetime(filtered_weather[['year', 'month', 'day']])    \n",
    "    daily_counts = filtered_weather.groupby('date').size()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    daily_counts.plot(kind='line', title='Number of Weather per Day for Selected Airports')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Rows')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "208559900255a239",
   "metadata": {},
   "source": [
    "It is clear that only a few dates during different periods have almost complete weather coverage in all three airports, which is not enough to rely on weather data when solving the problem. \n",
    "\n",
    "Let's observe if the delays depend on a particular carrier:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e2a10086860fd04",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=merged_df[merged_df['arr_delay'] < 100], x='carrier', y='arr_delay', palette=\"Set2\")\n",
    "\n",
    "plt.title(\"Arrival Delays by Carrier\")\n",
    "plt.xlabel(\"Carrier\")\n",
    "plt.ylabel(\"Delay (minutes)\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2e75a145581f055",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=merged_df[merged_df['dep_delay'] < 100], x='carrier', y='dep_delay', palette=\"Set2\", color=\"orange\")\n",
    "\n",
    "plt.title(\"Departure Delays by Carrier\")\n",
    "plt.xlabel(\"Carrier\")\n",
    "plt.ylabel(\"Delay (minutes)\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b154ba68c55d1fb6",
   "metadata": {},
   "source": [
    "There is a clear observable dependency between carriers and delays (we cropped the data not to include delays over 100 minutes for visual representation).\n",
    "\n",
    "Finally, seeing a correlation matrix for delays can be beneficial to see if there are any more dependencies worth investigating."
   ]
  },
  {
   "cell_type": "code",
   "id": "97f3afb381ac9642",
   "metadata": {},
   "source": [
    "corr_df = merged_df.select_dtypes(include=np.number).corr()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(data=corr_df[['arr_delay', 'dep_delay']], annot=True, cmap='YlGnBu')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9d4d53ba9d5dd2",
   "metadata": {},
   "source": [
    "Apart from weather conditions, which we considered unreliable, there is no meaningful correlation with other variables.\n",
    "\n",
    "## Cancellations\n",
    "We performed similar analysis for cancellations."
   ]
  },
  {
   "cell_type": "code",
   "id": "91a1cf3b7cba135a",
   "metadata": {},
   "source": [
    "flights_canceled = merged_df[merged_df['is_cancelled']]\n",
    "monthly_cancellations = flights_canceled.groupby('month').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_cancellations.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Flights Cancelled per Month', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Cancelled Flights', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=[f'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9dd57225-044a-48b6-bb82-05926a9cdab4",
   "metadata": {},
   "source": [
    "### Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "id": "6939e4d7-bc0e-4670-98a1-4e6f0e30e0d9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "182c4977-b6af-4eb1-9d0d-69a82f642ca2",
   "metadata": {},
   "source": [
    "def turn_into_regression_dataset(dataframe):\n",
    "    target_column = 'arr_delay'\n",
    "    additional_columns = ['dep_delay', 'temp', 'dewp', 'humid', 'precip', 'pressure']\n",
    "    dataframe = dataframe.drop(additional_columns, axis=1)\n",
    "    dataframe = dataframe.dropna()\n",
    "    datafram = dataframe.reset_index(drop=True)\n",
    "    X = dataframe.drop([target_column], axis=1)\n",
    "    y = dataframe[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=566)\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_features),  # Keep numerical features as is\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encode categorical features\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    X_train_processed = preprocessor.fit_transform(X_train).toarray()\n",
    "    X_test_processed = preprocessor.transform(X_test).toarray()\n",
    "    return (X_train_processed, y_train, X_test_processed, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a513cc-4a50-445e-b53f-474a5379a501",
   "metadata": {},
   "source": [
    "def SGDRegression(x_train, y_train, x_test, y_test):\n",
    "    model = SGDRegressor(\n",
    "                random_state=566\n",
    "    )\n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    epochs = 10\n",
    "    n_iterations = 10\n",
    "    batch_size = len(x_train) // n_iterations\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i in range(1, n_iterations + 1):\n",
    "            X_batch = x_train[(i - 1) * batch_size : i * batch_size]\n",
    "            y_batch = y_train[(i - 1) * batch_size : i * batch_size]\n",
    "    \n",
    "            model.partial_fit(X_batch, y_batch)\n",
    "    \n",
    "            y_train_pred = model.predict(X_batch)\n",
    "            y_test_pred = model.predict(x_test)\n",
    "    \n",
    "            training_loss = np.mean((y_batch - y_train_pred) ** 2)\n",
    "            test_loss = np.mean((y_test - y_test_pred) ** 2)\n",
    "    \n",
    "            training_losses.append(training_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            print(f\"Epoch: {epoch}, Iteration {i}: Training Loss = {training_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "    return training_losses, test_losses\n",
    "\n",
    "def regressionMLP(x_train, y_train, x_test, y_test):\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=566\n",
    "    )\n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    epochs = 10\n",
    "    n_iterations = 10\n",
    "    batch_size = len(x_train) // n_iterations\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i in range(1, n_iterations + 1):\n",
    "            X_batch = x_train[(i - 1) * batch_size : i * batch_size]\n",
    "            y_batch = y_train[(i - 1) * batch_size : i * batch_size]\n",
    "    \n",
    "            model.partial_fit(X_batch, y_batch)\n",
    "    \n",
    "            y_train_pred = model.predict(X_batch)\n",
    "            y_test_pred = model.predict(x_test)\n",
    "    \n",
    "            training_loss = np.mean((y_batch - y_train_pred) ** 2)\n",
    "            test_loss = np.mean((y_test - y_test_pred) ** 2)\n",
    "    \n",
    "            training_losses.append(training_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            print(f\"Epoch: {epoch}, Iteration {i}: Training Loss = {training_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "    return training_losses, test_losses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf59504e-e741-4000-a4d6-867eee1ce675",
   "metadata": {},
   "source": [
    "def plot_losses(model, training_losses, test_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(training_losses) + 1), training_losses, label=f'{model}: Training Loss', marker='o')\n",
    "    plt.plot(range(1, len(test_losses) + 1), test_losses, label=f'{model}: Test Loss', marker='o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Training and Test Loss Over Iterations')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f0f59d8-6844-448f-8829-b35eca9efd1d",
   "metadata": {},
   "source": [
    "#### EWR Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "035f76d3-0bdd-4666-81e5-d274a3727f9c",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_regression_dataset(ewr_delay_df)\n",
    "SGDLoss = SGDRegression(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"SGD\", *SGDLoss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55c73b62-5972-47e9-8fac-a49a4bd87278",
   "metadata": {},
   "source": [
    "mlp_losses = regressionMLP(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"MLP regression\", *mlp_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31b0c5af-d516-4304-bb9c-e2b06f6c32b0",
   "metadata": {},
   "source": [
    "#### JFK Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "818866b3-ffbf-4612-a890-7195b8ba1753",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_regression_dataset(jfk_delay_df)\n",
    "SGDLoss = SGDRegression(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"SGD\", *SGDLoss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39951e19-dbad-46cf-9b1b-64adeb4d7892",
   "metadata": {},
   "source": [
    "mlp_losses = regressionMLP(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"MLP regression\", *mlp_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9048cc68-763f-4542-a75d-9fdc59f49fcc",
   "metadata": {},
   "source": [
    "#### LGA Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "396fb3f4-0b78-40f4-926a-a641c4d77285",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_regression_dataset(lga_delay_df)\n",
    "SGDLoss = SGDRegression(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"SGD\", *SGDLoss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23e43219-a6aa-4e88-a2e5-4bedd47c6952",
   "metadata": {},
   "source": [
    "mlp_losses = regressionMLP(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"MLP regression\", *mlp_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a03453db-c092-46b0-8581-251c4dcd57ec",
   "metadata": {},
   "source": [
    "### Classification task"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ab02884-cf33-4474-a783-bc7632b7b12a",
   "metadata": {},
   "source": [
    "def turn_into_classification_dataset(dataframe):\n",
    "    target_column = 'is_cancelled'\n",
    "    additional_columns = ['temp', 'dewp', 'humid', 'precip', 'pressure']\n",
    "    dataframe = dataframe.drop(additional_columns, axis=1)\n",
    "    dataframe = dataframe.dropna()\n",
    "    datafram = dataframe.reset_index(drop=True)\n",
    "    X = dataframe.drop([target_column], axis=1)\n",
    "    y = dataframe[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=566)\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_features),  # Keep numerical features as is\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encode categorical features\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    X_train_processed = preprocessor.fit_transform(X_train).toarray()\n",
    "    X_test_processed = preprocessor.transform(X_test).toarray()\n",
    "    return (X_train_processed, y_train, X_test_processed, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa383552-3261-4810-b633-202eb5276324",
   "metadata": {},
   "source": [
    "def logReg(x_train, y_train, x_test, y_test):\n",
    "    model = LogisticRegression(\n",
    "                max_iter=500,\n",
    "                random_state=566,\n",
    "                warm_start=True\n",
    "            )\n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    epochs = 10\n",
    "    n_iterations = 10\n",
    "    batch_size = len(x_train) // n_iterations\n",
    "    for i in range(1, n_iterations + 1):\n",
    "        X_batch = x_train[(i - 1) * batch_size : i * batch_size]\n",
    "        y_batch = y_train[(i - 1) * batch_size : i * batch_size]\n",
    "    \n",
    "        model.fit(X_batch, y_batch)\n",
    "    \n",
    "        y_train_proba = model.predict_proba(X_batch)[:, 1]\n",
    "        y_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "            \n",
    "        training_loss = log_loss(y_batch, y_train_proba)\n",
    "        test_loss = log_loss(y_test, y_test_proba)\n",
    "    \n",
    "        training_losses.append(training_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Iteration {i}: Training Loss = {training_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "    return training_losses, test_losses, model.predict(x_test)\n",
    "\n",
    "def randForClass(x_train, y_train, x_test, y_test):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,         \n",
    "        max_depth=10,    \n",
    "        random_state=566,     \n",
    "        n_jobs=-1,\n",
    "        warm_start=True\n",
    "    )\n",
    "\n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    epochs = 10\n",
    "    n_iterations = 10\n",
    "    batch_size = len(x_train) // n_iterations\n",
    "    for i in range(1, n_iterations + 1):\n",
    "        X_batch = x_train[(i - 1) * batch_size : i * batch_size]\n",
    "        y_batch = y_train[(i - 1) * batch_size : i * batch_size]\n",
    "    \n",
    "        model.fit(X_batch, y_batch)\n",
    "        model.n_estimators += 10\n",
    "    \n",
    "        y_train_proba = model.predict_proba(X_batch)[:, 1]\n",
    "        y_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "            \n",
    "        training_loss = log_loss(y_batch, y_train_proba)\n",
    "        test_loss = log_loss(y_test, y_test_proba)\n",
    "    \n",
    "        training_losses.append(training_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Iteration {i}: Training Loss = {training_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "    return training_losses, test_losses, model.predict(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05158ee6-ee96-457d-bccc-da4695683900",
   "metadata": {},
   "source": [
    "#### EWR Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "24462345-b937-475f-8b1a-0224ddefd611",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_classification_dataset(ewr_cancel_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad3b54fb-2982-4b62-9f53-ebad881be6c0",
   "metadata": {},
   "source": [
    "LogLosses1, LogLosses2, _ = logReg(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Logistic Regression for classification\", LogLosses1, LogLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0dde213-e7f5-48ea-aeb9-f8019798d7ed",
   "metadata": {},
   "source": [
    "RandForLosses1, RandForLosses2, _ = randForClass(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Random Forests for classification\", RandForLosses1, RandForLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5daeb8a7-c687-4e38-a368-1e4d3def490c",
   "metadata": {},
   "source": [
    "#### JFK Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "1dac0db2-5248-4f97-a078-2b72506dcf95",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_classification_dataset(jfk_cancel_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e107876-c127-4277-a5a6-bfdf5e078351",
   "metadata": {},
   "source": [
    "LogLosses1, LogLosses2, _ = logReg(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Logistic Regression for classification\", LogLosses1, LogLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce7ac1c5-be80-4e9c-83eb-f62aa3a1aead",
   "metadata": {},
   "source": [
    "plot_losses(\"Logistic Regression for classification\", LogLosses1, LogLosses2)\n",
    "RandForLosses1, RandForLosses2, _ = randForClass(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Random Forests for classification\", RandForLosses1, RandForLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf33ee17-2702-49d0-9b2d-0c7383c45a7c",
   "metadata": {},
   "source": [
    "#### LGA Airport"
   ]
  },
  {
   "cell_type": "code",
   "id": "7db3f933-70cd-429d-9843-789a84a15339",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = turn_into_classification_dataset(jfk_cancel_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c094367a-9d82-4acd-b460-6b9d589ee711",
   "metadata": {},
   "source": [
    "LogLosses1, LogLosses2, y_pred = logReg(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Logistic Regression for classification\", LogLosses1, LogLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "647477b5-8f7a-4d2b-80e3-3410f4f5f493",
   "metadata": {},
   "source": [
    "RandForLosses1, RandForLosses2, y_pred2 = randForClass(x_train, y_train, x_test, y_test)\n",
    "plot_losses(\"Random Forests for classification\", RandForLosses1, RandForLosses2)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
